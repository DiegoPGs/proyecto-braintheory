{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final: Análisis de Datos del Human Connectome Project (HCP)\n",
    "\n",
    "En este proyecto, analizaremos datos de fMRI del Human Connectome Project (HCP) para predecir estados mentales y comportamientos usando redes neuronales. Utilizaremos PyTorch para construir y entrenar nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Importaciones Básicas ====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "# ==== Importaciones de PyTorch ====\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ==== Importaciones Adicionales ====\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ==== Configuración de Visualización ====\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# ==== Configuración Global ====\n",
    "# Constantes para el dataset HCP\n",
    "CONSTANTS = {\n",
    "    'N_SUBJECTS': 100,\n",
    "    'N_PARCELS': 360,\n",
    "    'TR': 0.72,  # Resolución temporal en segundos\n",
    "    'HEMIS': [\"Right\", \"Left\"],\n",
    "    'RUNS': ['LR', 'RL'],\n",
    "    'N_RUNS': 2,\n",
    "}\n",
    "\n",
    "# Definición de experimentos y condiciones\n",
    "EXPERIMENTS = {\n",
    "    'MOTOR': {\n",
    "        'cond': ['lf', 'rf', 'lh', 'rh', 't', 'cue'],\n",
    "        'description': 'Movimientos de pie izquierdo, derecho, mano izquierda, derecha, lengua y señal'\n",
    "    },\n",
    "    'GAMBLING': {\n",
    "        'cond': ['loss', 'win'],\n",
    "        'description': 'Respuestas a pérdidas y ganancias en juegos de azar'\n",
    "    },\n",
    "    'EMOTION': {\n",
    "        'cond': ['fear', 'neut'],\n",
    "        'description': 'Respuestas a expresiones de miedo y neutrales'\n",
    "    },\n",
    "    # ... [otros experimentos]\n",
    "}\n",
    "\n",
    "# Configuración de semilla aleatoria para reproducibilidad\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Configura las semillas aleatorias para reproducibilidad.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): Valor de la semilla aleatoria\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "# Configuración de dispositivo para PyTorch\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Configuración de rutas\n",
    "HCP_DIR = \"./hcp_task\"\n",
    "if not os.path.exists(HCP_DIR):\n",
    "    os.makedirs(HCP_DIR)\n",
    "\n",
    "# TODO: Sustituir con el setup en la libreta de proyecto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del Modelo\n",
    "\n",
    "Definimos nuestro modelo LSTM para predecir estados mentales y comportamientos a partir de los datos de fMRI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, n_features: int, n_timesteps: int, n_hidden: int, n_layers: int):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(n_features, n_hidden, n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(n_hidden, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.n_hidden).to(DEVICE)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.n_hidden).to(DEVICE)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class LSTMRegression2(nn.Module):\n",
    "    def __init__(self, n_features: int, n_timesteps: int, n_hidden: int, n_layers: int):\n",
    "        super(LSTMRegression2, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(n_features, n_hidden, n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(n_hidden, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.n_hidden).to(DEVICE)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.n_hidden).to(DEVICE)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Entrenamiento Optimizada\n",
    "\n",
    "Reescribimos la función de entrenamiento utilizando la versión optimizada del archivo `optimized_training.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(experiment: str, regions: List[int], label_type: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Implementar la lógica para cargar y preparar los datos\n",
    "    pass\n",
    "\n",
    "def optimized_training(\n",
    "    experiment: str,\n",
    "    regions: List[int],\n",
    "    model_params: Dict[str, int],\n",
    "    training_params: Dict[str, Union[float, int, str]],\n",
    "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ") -> Tuple[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Versión optimizada de la función de entrenamiento para modelos LSTM.\n",
    "    \n",
    "    Args:\n",
    "        experiment (str): Nombre del experimento ('GAMBLING', 'MOTOR', etc.)\n",
    "        regions (List[int]): Lista de índices de regiones cerebrales a utilizar\n",
    "        model_params (Dict): Parámetros del modelo como:\n",
    "            - n_features: Número de características de entrada\n",
    "            - n_timesteps: Número de pasos temporales\n",
    "            - n_hidden: Dimensiones del estado oculto\n",
    "            - n_layers: Número de capas LSTM\n",
    "        training_params (Dict): Parámetros de entrenamiento como:\n",
    "            - lr: Learning rate\n",
    "            - batch_size: Tamaño del batch\n",
    "            - n_epochs: Número de épocas\n",
    "            - label_type: Tipo de etiqueta a predecir\n",
    "        device (torch.device): Dispositivo para entrenar (CPU/GPU)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, float]: DataFrame con métricas de entrenamiento y tiempo total\n",
    "    \"\"\"\n",
    "    # Inicializar temporizador\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Inicializar modelo y moverlo al dispositivo correcto\n",
    "    if training_params['label_type'] == 'flanker':\n",
    "        model = LSTMRegression2(**model_params).to(device)\n",
    "    else:\n",
    "        model = LSTMRegression(**model_params).to(device)\n",
    "    \n",
    "    # Inicializar optimizador y criterio\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=training_params['lr'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Inicializar scaler para precisión mixta\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Cargar y preparar datos\n",
    "    X, y = load_and_prepare_data(experiment, regions, training_params['label_type'])\n",
    "    \n",
    "    # Convertir datos a tensores y moverlos a GPU si está disponible\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Dividir en train y test\n",
    "    train_size = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # Crear DataLoader para procesamiento por lotes eficiente\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=training_params['batch_size'],\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Listas para almacenar métricas\n",
    "    metrics = []\n",
    "    \n",
    "    # Loop principal de entrenamiento\n",
    "    for epoch in tqdm(range(training_params['n_epochs']), desc=\"Entrenamiento\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Usar precisión mixta para acelerar el entrenamiento en GPU\n",
    "            with autocast():\n",
    "                output = model(batch_X)\n",
    "                loss = criterion(output.view(-1), batch_y)\n",
    "                \n",
    "                if training_params['label_type'] in ['WL', 'gender']:\n",
    "                    accuracy = ((output.view(-1) > 0.5) == batch_y).float().mean()\n",
    "            \n",
    "            # Backward pass con scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Evaluación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(X_test)\n",
    "            test_loss = criterion(y_hat.view(-1), y_test)\n",
    "            \n",
    "            if training_params['label_type'] in ['WL', 'gender']:\n",
    "                test_accuracy = ((y_hat.view(-1) > 0.5) == y_test).float().mean()\n",
    "        \n",
    "        # Guardar métricas\n",
    "        metrics_dict = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss / len(train_loader),\n",
    "            'test_loss': test_loss.item()\n",
    "        }\n",
    "        \n",
    "        if training_params['label_type'] in ['WL', 'gender']:\n",
    "            metrics_dict.update({\n",
    "                'train_accuracy': accuracy.item(),\n",
    "                'test_accuracy': test_accuracy.item()\n",
    "            })\n",
    "            \n",
    "        metrics.append(metrics_dict)\n",
    "    \n",
    "    # Calcular tiempo total\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return pd.DataFrame(metrics), total_time\n",
    "\n",
    "# Definir parámetros\n",
    "model_params = {\n",
    "    'n_features': 360,\n",
    "    'n_timesteps': 67,\n",
    "    'n_hidden': 20,\n",
    "    'n_layers': 3\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "    'lr': 1e-3,\n",
    "    'batch_size': 16,  # Ajustar según memoria disponible\n",
    "    'n_epochs': 50,\n",
    "    'label_type': 'WL'\n",
    "}\n",
    "\n",
    "# Entrenar modelo\n",
    "metrics_df, training_time = optimized_training(\n",
    "    experiment='GAMBLING',\n",
    "    regions=list(range(360)),\n",
    "    model_params=model_params,\n",
    "    training_params=training_params\n",
    ")\n",
    "\n",
    "print(f\"Tiempo total de entrenamiento: {training_time:.2f} segundos\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
