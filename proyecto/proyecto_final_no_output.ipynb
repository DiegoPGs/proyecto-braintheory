{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Jia7jgNZJO9t"
   },
   "source": [
    "# Load HCP parcellated task data\n",
    "## (version with Behavioural Data)\n",
    "\n",
    "The HCP dataset comprises task-based fMRI from a large sample of human subjects. The NMA-curated dataset includes time series data that has been preprocessed and spatially-downsampled by aggregating within 360 regions of interest.\n",
    "\n",
    "In order to use this dataset, please electronically sign the HCP data use terms at [ConnectomeDB](https://db.humanconnectome.org). Instructions for this are on pp. 24-25 of the [HCP Reference Manual](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf).\n",
    "\n",
    "In this notebook, NMA provides code for downloading the data and doing some basic visualisation and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {},
    "id": "V4Xn5sZ8JO9z",
    "outputId": "c4dd3931-3cc7-48d6-ee72-133a6c7ca45e"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install nilearn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {},
    "id": "Witlz4leJO91"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ktWB1mJco_Em"
   },
   "outputs": [],
   "source": [
    "# Necessary for loading data and modeling in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "I6Qd_MJ6JO91"
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {},
    "id": "6FRA0vNHJO92"
   },
   "outputs": [],
   "source": [
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 100\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasser parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in seconds\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated twice in each subject\n",
    "RUNS   = ['LR','RL']\n",
    "N_RUNS = 2\n",
    "\n",
    "# There are 7 tasks. Each has a number of 'conditions'\n",
    "# TIP: look inside the data folders for more fine-graned conditions\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
    "    'WM'         : {'cond':['0bbody','0bfaces','0bplaces','0btools','2bbody','2bfaces','2bplaces','2btools']},\n",
    "    'EMOTION'    : {'cond':['fear','neut']},\n",
    "    'GAMBLING'   : {'cond':['loss','win']},\n",
    "    'LANGUAGE'   : {'cond':['math','story']},\n",
    "    'RELATIONAL' : {'cond':['match','relation']},\n",
    "    'SOCIAL'     : {'cond':['ment','rnd']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "ZCu-Z0Y_JO93"
   },
   "source": [
    "> For a detailed description of the tasks have a look pages 45-54 of the [HCP reference manual](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "7TQa_85dJO94"
   },
   "source": [
    "# Downloading data\n",
    "\n",
    "The task data are shared in different files, but they will unpack into the same directory structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {},
    "id": "2Hzt35OdJO95"
   },
   "outputs": [],
   "source": [
    "# @title Download data file\n",
    "import os, requests\n",
    "\n",
    "fname = \"hcp_task.tgz\"\n",
    "url = \"https://osf.io/2y3fw/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0JVeU-4-a4EO"
   },
   "outputs": [],
   "source": [
    "# # @title Download the data\n",
    "\n",
    "# # @markdown Task data in `HCP_DIR/hcp_task`, rest in `HCP_DIR/hcp_rest`, covariate in `HCP_DIR/hcp`\n",
    "\n",
    "# import os, requests, tarfile\n",
    "\n",
    "# fnames = [\"hcp_rest.tgz\",\n",
    "#           \"hcp_task.tgz\",\n",
    "#           \"hcp_covariates.tgz\",\n",
    "#           \"atlas.npz\"]\n",
    "# urls = [\"https://osf.io/bqp7m/download\",\n",
    "#         \"https://osf.io/s4h8j/download\",\n",
    "#         \"https://osf.io/x5p4g/download\",\n",
    "#         \"https://osf.io/j5kuc/download\"]\n",
    "\n",
    "# for fname, url in zip(fnames, urls):\n",
    "#   if not os.path.isfile(fname):\n",
    "#     try:\n",
    "#       r = requests.get(url)\n",
    "#     except requests.ConnectionError:\n",
    "#       print(\"!!! Failed to download data !!!\")\n",
    "#     else:\n",
    "#       if r.status_code != requests.codes.ok:\n",
    "#         print(\"!!! Failed to download data !!!\")\n",
    "#       else:\n",
    "#         print(f\"Downloading {fname}...\")\n",
    "#         with open(fname, \"wb\") as fid:\n",
    "#           fid.write(r.content)\n",
    "#         print(f\"Download {fname} completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {},
    "id": "sn0lKtSeJO96"
   },
   "outputs": [],
   "source": [
    "# The download cells will store the data in nested directories starting here:\n",
    "HCP_DIR = \"./hcp_task\"\n",
    "\n",
    "# importing the \"tarfile\" module\n",
    "import tarfile\n",
    "\n",
    "# open file\n",
    "with tarfile.open(fname) as tfile:\n",
    "  # extracting file\n",
    "  tfile.extractall('.')\n",
    "\n",
    "subjects = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GwSDz7NWa8Xe"
   },
   "outputs": [],
   "source": [
    "# # @title Extract the data in `HCP_DIR`\n",
    "# fnames = [\"hcp_covariates\", \"hcp_rest\", \"hcp_task\"]\n",
    "\n",
    "# for fname in fnames:\n",
    "#   # open file\n",
    "#   path_name = os.path.join(HCP_DIR, fname)\n",
    "#   if not os.path.exists(path_name):\n",
    "#     print(f\"Extracting {fname}.tgz...\")\n",
    "#     with tarfile.open(f\"{fname}.tgz\") as fzip:\n",
    "#       fzip.extractall(HCP_DIR)\n",
    "#   else:\n",
    "#     print(f\"File {fname}.tgz has already been extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "hLXqJ7mUJO96"
   },
   "source": [
    "## Understanding the folder organisation\n",
    "\n",
    "The data folder has the following organisation:\n",
    "\n",
    "- hcp\n",
    "  - regions.npy (information on the brain parcellation)\n",
    "  - subjects_list.txt (list of subject IDs)\n",
    "  - subjects (main data folder)\n",
    "    - [subjectID] (subject-specific subfolder)\n",
    "      - EXPERIMENT (one folder per experiment)\n",
    "        - RUN (one folder per run)\n",
    "          - data.npy (the parcellated time series data)\n",
    "          - EVs (EVs folder)\n",
    "            - [ev1.txt] (one file per condition)\n",
    "            - [ev2.txt]\n",
    "            - Stats.txt (behavioural data [where available] - averaged per run)\n",
    "            - Sync.txt (ignore this file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "kv__YxLCJO97"
   },
   "source": [
    "## Loading region information\n",
    "\n",
    "Downloading this dataset will create the `regions.npy` file, which contains the region name and network assignment for each parcel.\n",
    "\n",
    "Detailed information about the name used for each region is provided [in the Supplement](https://static-content.springer.com/esm/art%3A10.1038%2Fnature18933/MediaObjects/41586_2016_BFnature18933_MOESM330_ESM.pdf) to [Glasser et al. 2016](https://www.nature.com/articles/nature18933).\n",
    "\n",
    "Information about the network parcellation is provided in [Ji et al, 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6289683/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {},
    "id": "VAnhyg1bJO97"
   },
   "outputs": [],
   "source": [
    "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
    "region_info = dict(\n",
    "    name=regions[0].tolist(),\n",
    "    network=regions[1],\n",
    "    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "OD2vIz1IJO98"
   },
   "source": [
    "# Help functions\n",
    "\n",
    "We provide two helper functions: one for loading the time series from a single suject and a single run, and one for loading an EV file for each task.\n",
    "\n",
    "An EV file (EV:Explanatory Variable) describes the task experiment in terms of stimulus onset, duration, and amplitude. These can be used to model the task time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {},
    "id": "C4IRRVuCJO98"
   },
   "outputs": [],
   "source": [
    "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "\n",
    "  Args:\n",
    "    subject (str):      subject ID to load\n",
    "    experiment (str):   Name of experiment\n",
    "    run (int):          (0 or 1)\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_run  = RUNS[run]\n",
    "  bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
    "  bold_file = \"data.npy\"\n",
    "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "  return ts\n",
    "\n",
    "\n",
    "def load_evs(subject, experiment, run):\n",
    "  \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "  Args:\n",
    "    subject (str): subject ID to load\n",
    "    experiment (str) : Name of experiment\n",
    "    run (int): 0 or 1\n",
    "\n",
    "  Returns\n",
    "    evs (list of lists): A list of frames associated with each condition\n",
    "\n",
    "  \"\"\"\n",
    "  frames_list = []\n",
    "  taskey = f'tfMRI_{experiment}_{RUNS[run]}'\n",
    "  for cond in EXPERIMENTS[experiment]['cond']:\n",
    "    ev_file  = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{taskey}/EVs/{cond}.txt\"\n",
    "    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "    ev       = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "    # Determine when trial starts, rounded down\n",
    "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "    # Use trial duration to determine how many frames to include for trial\n",
    "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "    # Take the range of frames that correspond to this specific trial\n",
    "    frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "    frames_list.append(frames)\n",
    "\n",
    "  return frames_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "N0_dt7GyJO99"
   },
   "source": [
    "# Example run\n",
    "\n",
    "Let's load the timeseries data for the MOTOR experiment from a single subject and a single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {},
    "id": "AgdLtrt4JO99",
    "outputId": "d46ca68c-c27b-4979-ea1e-ce33288f04ff"
   },
   "outputs": [],
   "source": [
    "my_exp = 'GAMBLING'\n",
    "my_subj = subjects[1]\n",
    "my_run = 1\n",
    "\n",
    "data = load_single_timeseries(subject=my_subj,\n",
    "                              experiment=my_exp,\n",
    "                              run=my_run,\n",
    "                              remove_mean=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "XVr0L_Y5JO9-"
   },
   "source": [
    "As you can see the time series data contains 284 time points in 360 regions of interest (ROIs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "wVN2rmfHJO9-"
   },
   "source": [
    "Now in order to understand how to model these data, we need to relate the time series to the experimental manipulation. This is described by the EV files. Let us load the EVs for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {},
    "id": "JxqejTN5JO9-"
   },
   "outputs": [],
   "source": [
    "evs = load_evs(subject=my_subj, experiment=my_exp, run=my_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "dG3zRezFUNx0",
    "outputId": "1bbb29a0-9d18-4fb6-849b-8ee63b3ee7bc"
   },
   "outputs": [],
   "source": [
    "my_exp = 'GAMBLING'\n",
    "my_regions = [*range(360)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Pv9teAChyBN",
    "outputId": "d8f423e0-7ccc-4cf6-b55e-758ec98c1794"
   },
   "outputs": [],
   "source": [
    "def get_subject_gender(subject_id: str, dataframe: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Determina el género de un sujeto y lo codifica como valor binario.\n",
    "    \n",
    "    Args:\n",
    "        subject_id (str): ID del sujeto en el formato del HCP dataset\n",
    "        dataframe (pd.DataFrame): DataFrame que contiene la información demográfica\n",
    "                                Debe tener columnas 'Subject_ID' y 'Gender'\n",
    "    \n",
    "    Returns:\n",
    "        int: 1 para género femenino ('F'), 0 para género masculino ('M')\n",
    "        \n",
    "    Raises:\n",
    "        KeyError: Si el subject_id no se encuentra en el DataFrame\n",
    "        ValueError: Si el género no es 'F' o 'M'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertir subject_id a int para matching con el DataFrame\n",
    "        subject_int = int(subject_id)\n",
    "        \n",
    "        # Obtener las filas que coinciden con el subject_id\n",
    "        matches = dataframe[dataframe['Subject_ID'] == subject_int]\n",
    "        \n",
    "        if len(matches) == 0:\n",
    "            raise KeyError(f\"Subject ID {subject_id} no encontrado en el DataFrame\")\n",
    "            \n",
    "        if len(matches) > 1:\n",
    "            print(f\"Advertencia: Múltiples entradas para Subject ID {subject_id}\")\n",
    "            \n",
    "        # Obtener el género del primer match\n",
    "        gender = matches['Gender'].iloc[0]\n",
    "        \n",
    "        # Validar y codificar el género\n",
    "        if gender not in ['F', 'M']:\n",
    "            raise ValueError(f\"Género inválido: {gender}\")\n",
    "            \n",
    "        return 1 if gender == 'F' else 0\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error al procesar subject_id {subject_id}: {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado al procesar subject_id {subject_id}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# gender_list = []\n",
    "# for i, subject in enumerate(subjects):\n",
    "#     gender = get_subject_gender(subject, HCP_data_frame)\n",
    "#     gender_list.append(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "TLeGbU6oNilE",
    "outputId": "71ce2c32-961b-44a4-b840-dad16250bf1f"
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(evs)\n",
    "print(len(evs))\n",
    "print(len(evs[0]))\n",
    "\n",
    "print(data[:,evs[0][0]].shape)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(data[[164, 344],:][:,evs[0][0]].T)\n",
    "axs[0, 1].plot(data[[164, 344],:][:,evs[0][1]].T)\n",
    "axs[1, 0].plot(data[[164, 344],:][:,evs[1][0]].T)\n",
    "axs[1, 1].plot(data[[164, 344],:][:,evs[1][1]].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu8QvwraYVjW"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2c7aabmdqZHT"
   },
   "outputs": [],
   "source": [
    "class LSTMRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,n_features,seq_length, n_hidden, n_layers):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = n_hidden # number of hidden states 3 35 20\n",
    "        self.n_layers = n_layers # number of LSTM layers (stacked) 7 2 4\n",
    "\n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features,\n",
    "                                    hidden_size = self.n_hidden,\n",
    "                                    num_layers = self.n_layers,\n",
    "                                    batch_first = True, dropout=.3)\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x, (self.hidden, _) = self.l_lstm(x)\n",
    "        return torch.sigmoid(self.l_linear(x[:,-1,:]))\n",
    "        # return self.l_linear(x[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "A6WLqjI60U08"
   },
   "outputs": [],
   "source": [
    "class LSTMRegressionCond(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,n_features,seq_length, n_hidden, n_layers, n_add, n_add_hidden):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.n_features   = n_features\n",
    "        self.seq_len      = seq_length\n",
    "        self.n_hidden     = n_hidden # number of hidden states 3 35 20\n",
    "        self.n_layers     = n_layers # number of LSTM layers (stacked) 7 2 4\n",
    "        self.n_add        = n_add\n",
    "        self.n_add_hidden = n_add_hidden\n",
    "\n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features,\n",
    "                                    hidden_size = self.n_hidden,\n",
    "                                    num_layers = self.n_layers,\n",
    "                                    batch_first = True, dropout=.3)\n",
    "        self.fc_add = torch.nn.Linear(self.n_add, self.n_add_hidden)\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden+self.n_add_hidden, 1)\n",
    "\n",
    "    def forward(self, x, x_add):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x_add_out = self.fc_add(x_add)\n",
    "        x, (self.hidden, _) = self.l_lstm(x)\n",
    "        return torch.sigmoid( self.l_linear( torch.cat( (x_add_out, x[:,-1,:]), dim=1 )))\n",
    "        # return self.l_linear(x[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ELZHojpP5XGV"
   },
   "outputs": [],
   "source": [
    "class LSTMRegressionCond2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,n_features,seq_length, n_hidden, n_layers, n_add, n_add_hidden):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.n_features   = n_features\n",
    "        self.seq_len      = seq_length\n",
    "        self.n_hidden     = n_hidden # number of hidden states 3 35 20\n",
    "        self.n_layers     = n_layers # number of LSTM layers (stacked) 7 2 4\n",
    "        self.n_add        = n_add\n",
    "        self.n_add_hidden = n_add_hidden\n",
    "\n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features,\n",
    "                                    hidden_size = self.n_hidden,\n",
    "                                    num_layers = self.n_layers,\n",
    "                                    batch_first = True, dropout=.3)\n",
    "        self.fc_add = torch.nn.Linear(self.n_add, self.n_add_hidden)\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden+self.n_add_hidden, 1)\n",
    "\n",
    "    def forward(self, x, x_add):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x_add_out = self.fc_add(x_add)\n",
    "        x, (self.hidden, _) = self.l_lstm(x)\n",
    "        return self.l_linear( torch.cat( (x_add_out, x[:,-1,:]), dim=1 ))\n",
    "        # return self.l_linear(x[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kZgYrESJZ-CN"
   },
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "q8ZIrzkuNS7M"
   },
   "outputs": [],
   "source": [
    "class LSTMRegression2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,n_features,seq_length, n_hidden, n_layers):\n",
    "        super(LSTMRegression2, self).__init__()\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = n_hidden # number of hidden states 3 35 20\n",
    "        self.n_layers = n_layers # number of LSTM layers (stacked) 7 2 4\n",
    "\n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features,\n",
    "                                    hidden_size = self.n_hidden,\n",
    "                                    num_layers = self.n_layers,\n",
    "                                    batch_first = True, dropout=.3)\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x, (self.hidden, _) = self.l_lstm(x)\n",
    "        return self.l_linear(x[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "d3wO5aOmISnW"
   },
   "outputs": [],
   "source": [
    "def load_data(my_exp , my_regions):\n",
    "\n",
    "  HCP_data_frame = pd.read_csv('ddisc.csv')\n",
    "\n",
    "  steps = int(10/TR+.5)    # calculating stpes for 10 seconds\n",
    "\n",
    "  ts_list = []\n",
    "  target_list = [] # wininf or loosing\n",
    "\n",
    "  DDisc_AUC_200_list = []\n",
    "  DDisc_AUC_40K_list = []\n",
    "  Flanker_Unadj_list = []\n",
    "  Gender_list = []\n",
    "\n",
    "  order = torch.randperm(100).numpy()\n",
    "  for i in order:     # subject\n",
    "    for r in range(2):     # run\n",
    "\n",
    "      data = load_single_timeseries(subject=subjects[i],\n",
    "                                    experiment=my_exp,\n",
    "                                    run=r,\n",
    "                                    remove_mean=True)\n",
    "\n",
    "      # Normalizing data by subject\n",
    "      timeseries = (data - data.mean()) / data.std()\n",
    "\n",
    "      # get the evs plus 10 seconds of the time series before and after (14 steps for tr=.76)\n",
    "      evs = load_evs(subject=subjects[i], experiment=my_exp, run=r)\n",
    "\n",
    "      for c in range(2):   # condiction\n",
    "        for t in range(2): # block\n",
    "          evs_index = np.hstack((\n",
    "            np.arange(evs[c][t][0]-steps,evs[c][t][0]),\n",
    "            np.array(evs[c][t]),\n",
    "            np.arange((evs[c][t][-1]+1),(evs[c][t][-1]+1+steps))\n",
    "          ))\n",
    "\n",
    "          # (N, L, H_in)\n",
    "\n",
    "          ts_list.append(timeseries[my_regions,:][:,evs_index].T)\n",
    "          target_list.append(c)\n",
    "\n",
    "          DDisc_AUC_200_list.append(HCP_data_frame[HCP_data_frame['Subject_ID']==int(subjects[i])]['DDisc_AUC_200'])\n",
    "          DDisc_AUC_40K_list.append(HCP_data_frame[HCP_data_frame['Subject_ID']==int(subjects[i])]['DDisc_AUC_40K'])\n",
    "          Flanker_Unadj_list.append(HCP_data_frame[HCP_data_frame['Subject_ID']==int(subjects[i])]['Flanker_Unadj'])\n",
    "          Gender_list.append(1 if HCP_data_frame[HCP_data_frame['Subject_ID']==int(subjects[i])]['Gender'].item()=='F' else 0)\n",
    "\n",
    "  # flanker = torch.tensor(np.array(Flanker_Unadj_list)).squeeze(1)\n",
    "  # flanker = (flanker-flanker.min())/(flanker-flanker.min()).max()\n",
    "\n",
    "  return (\n",
    "      torch.tensor(np.array(ts_list)),\n",
    "      torch.tensor(np.array(target_list)),\n",
    "      torch.tensor(np.array(DDisc_AUC_200_list)).squeeze(1),\n",
    "      torch.tensor(np.array(DDisc_AUC_40K_list)).squeeze(1),\n",
    "      torch.tensor(np.array(Flanker_Unadj_list)).squeeze(1),\n",
    "      torch.tensor(np.array(Gender_list))\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5p9EmV9RZq-K"
   },
   "outputs": [],
   "source": [
    "def training(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type):\n",
    "\n",
    "  if label_type=='flanker':\n",
    "    net = LSTMRegression2(n_features,n_timesteps, n_hidden, n_layers)\n",
    "  else:\n",
    "    net = LSTMRegression(n_features,n_timesteps, n_hidden, n_layers)\n",
    "\n",
    "  criterion =   torch.nn.MSELoss() # torch.nn.MSELoss() # reduction='sum' created huge loss value nn.CrossEntropyLoss() # torch.nn.NLLLoss() #\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "  label = {}\n",
    "  X, label['WL'], label['200'], label['40k'], label['flanker'], label['gender'] = load_data(experiment, regions)\n",
    "  y = label[label_type]\n",
    "  batch_size = 640\n",
    "\n",
    "  X_train = X[0:640,:,:]\n",
    "  y_train = y[0:640]\n",
    "\n",
    "  X_test = X[640:800,:,:]\n",
    "  y_test = y[640:800]\n",
    "\n",
    "  df_list = []\n",
    "\n",
    "  net.train()\n",
    "\n",
    "  for t in range(train_episodes):\n",
    "    for b in range(0,len(X_train),batch_size):\n",
    "\n",
    "      inpt = X_train[b:b+batch_size,:,:]\n",
    "      target = y_train[b:b+batch_size]\n",
    "\n",
    "      x_batch = inpt.clone().float()\n",
    "      y_batch = target.clone().float()\n",
    "\n",
    "      output = net(x_batch)\n",
    "      # if label_type=='flanker':\n",
    "      #   output=torch.log(output/(1-output+1e-8))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = criterion(output.view(-1), y_batch)\n",
    "      accuracy=((output.view(-1)>.5)==y_batch).sum()/640\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      net.eval()\n",
    "      y_hat = net(X_test.float())\n",
    "      # if label_type=='flanker':\n",
    "      #   y_hat=torch.log(y_hat/(1-y_hat+1e-8))\n",
    "\n",
    "      loss_val = criterion(y_hat.view(-1), y_test.float())\n",
    "      accuracy_val=((y_hat.view(-1)>.5)==y_test).sum()/(800-640)\n",
    "\n",
    "      if label_type=='WL' or label_type=='gender':\n",
    "        df_list.append({'epoc': t, 'accuracy': accuracy.item(), 'train':True})\n",
    "        df_list.append({'epoc': t, 'accuracy': accuracy_val.item(), 'train':False})\n",
    "      else:\n",
    "        df_list.append({'epoc': t, 'loss': loss.item(), 'train':True})\n",
    "        df_list.append({'epoc': t, 'loss': loss_val.item(), 'train':False})\n",
    "\n",
    "  return pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aN4Dmq0EdGhN",
    "outputId": "42f8b4a0-a3e4-4252-e373-056b9caaed5c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment='GAMBLING'\n",
    "regions = [*range(360)]\n",
    "\n",
    "n_features     = 360  # this is number of parallel inputs\n",
    "n_timesteps    = 67   # this is number of timesteps\n",
    "n_hidden       = 20\n",
    "n_layers       = 3\n",
    "label_type     = 'WL'\n",
    "\n",
    "lr             = 1e-3\n",
    "train_episodes = 50\n",
    "\n",
    "n_trials       = 10\n",
    "\n",
    "df_list = []\n",
    "for i in range(n_trials):\n",
    "  df_list.append(training(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type))\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True) #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "33q3paROf19B",
    "outputId": "5862bcde-83e5-49a5-df21-f654df891441"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoc', y='accuracy', data=df, hue='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOZa5_QisjFd",
    "outputId": "89a38022-3224-46db-973d-97e2adc39827"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment='GAMBLING'\n",
    "regions = [*range(360)]\n",
    "\n",
    "n_features     = 360  # this is number of parallel inputs\n",
    "n_timesteps    = 67   # this is number of timesteps\n",
    "n_hidden       = 20\n",
    "n_layers       = 3\n",
    "label_type     = '200'\n",
    "\n",
    "lr             = 1e-3\n",
    "train_episodes = 50\n",
    "\n",
    "n_trials       = 10\n",
    "\n",
    "df_list = []\n",
    "for i in range(n_trials):\n",
    "  df_list.append(training(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type))\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True) #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "OiUfc7zHsvOK",
    "outputId": "87db0d62-b1ec-4669-bea0-89afb5ad9639"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoc', y='loss', data=df, hue='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUHvBVWhw0If",
    "outputId": "4e4a4067-a2ce-4c9a-dd23-7e4ef5dd3c33"
   },
   "outputs": [],
   "source": [
    "print('RMSE Testin set = ',round(df[df['train']==False]['loss'].min()**(1/2),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRJfjPfcyTvt",
    "outputId": "1cd18bf4-e34d-41ff-c5cd-b158d858a92a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment='GAMBLING'\n",
    "regions = [*range(360)]\n",
    "\n",
    "n_features     = 360  # this is number of parallel inputs\n",
    "n_timesteps    = 67   # this is number of timesteps\n",
    "n_hidden       = 20\n",
    "n_layers       = 3\n",
    "label_type     = '40k'\n",
    "\n",
    "lr             = 1e-3\n",
    "train_episodes = 50\n",
    "\n",
    "n_trials       = 10\n",
    "\n",
    "df_list = []\n",
    "for i in range(n_trials):\n",
    "  df_list.append(training(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type))\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True) #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "d1aI5wVZyrF-",
    "outputId": "66ac31e1-efa1-4572-d3c2-501e87288522"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoc', y='loss', data=df, hue='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1-Y54Fbywdq",
    "outputId": "7b7dac45-afa7-43bc-c378-e945512cb58e"
   },
   "outputs": [],
   "source": [
    "print('RMSE Testin set = ',round(df[df['train']==False]['loss'].min()**(1/2),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af-dMXUE1Owu",
    "outputId": "1121cf7b-5cf8-46f2-90d3-47ed91693120"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment='GAMBLING'\n",
    "regions = [*range(360)]\n",
    "\n",
    "n_features     = 360  # this is number of parallel inputs\n",
    "n_timesteps    = 67   # this is number of timesteps\n",
    "n_hidden       = 20\n",
    "n_layers       = 3\n",
    "label_type     = 'flanker'\n",
    "\n",
    "lr             = 1e-1\n",
    "train_episodes = 500\n",
    "\n",
    "n_trials       = 1\n",
    "\n",
    "df_list = []\n",
    "for i in range(n_trials):\n",
    "  df_list.append(training(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type))\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True) #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "Ss2YY4yl1fnI",
    "outputId": "07dbe21b-eaca-4421-80c5-46baade61a66"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoc', y='loss', data=df, hue='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDT_V6pK1o0B",
    "outputId": "27abf368-a81e-4638-c07d-ba913e25008c"
   },
   "outputs": [],
   "source": [
    "print('RMSE Testin set = ',round(df[df['train']==False]['loss'].min()**(1/2),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBWZD-utTWOO",
    "outputId": "a1e62506-a40c-47d1-ee65-52bd106ad2b7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment='GAMBLING'\n",
    "regions = [*range(360)]\n",
    "\n",
    "n_features     = 360  # this is number of parallel inputs\n",
    "n_timesteps    = 67   # this is number of timesteps\n",
    "n_hidden       = 20\n",
    "n_layers       = 3\n",
    "label_type     = 'flanker'\n",
    "\n",
    "lr             = 1e-1\n",
    "train_episodes = 150\n",
    "\n",
    "n_trials       = 10\n",
    "\n",
    "df_list = []\n",
    "for i in range(n_trials):\n",
    "  df_list.append(training(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type))\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True) #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "8kNUmKJyTjV5",
    "outputId": "25265dda-1780-43b4-9e69-3ac8ebe7f39b"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoc', y='loss', data=df, hue='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEg69IPzTqwg",
    "outputId": "248a2e9e-589d-4254-a696-dfaa5aabd8e1"
   },
   "outputs": [],
   "source": [
    "print('RMSE Testin set = ',round(df[df['train']==False]['loss'].min()**(1/2),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQ1SZgFUUu2k",
    "outputId": "a997f165-07ab-41c3-fcb7-ec8d26355d59"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment='GAMBLING'\n",
    "regions = [*range(360)]\n",
    "\n",
    "n_features     = 360  # this is number of parallel inputs\n",
    "n_timesteps    = 67   # this is number of timesteps\n",
    "n_hidden       = 20\n",
    "n_layers       = 3\n",
    "label_type     = 'gender'\n",
    "\n",
    "lr             = 1e-3\n",
    "train_episodes = 50\n",
    "\n",
    "n_trials       = 10\n",
    "\n",
    "df_list = []\n",
    "for i in range(n_trials):\n",
    "  df_list.append(training(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type))\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True) #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "jNWDWdFpWEc8",
    "outputId": "74fa9df0-cd5a-41c3-ff12-6a9804e05c7d"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoc', y='accuracy', data=df, hue='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJpTrGfAWEtL",
    "outputId": "f0d1795a-2449-4a3c-941f-e67421e768c9"
   },
   "outputs": [],
   "source": [
    "print('Accuracy Test set = ',round(df[df['train']==False]['accuracy'].max(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acEjjBjrXsBm",
    "outputId": "4b20ccd8-c182-4f61-8821-344737dd411a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment='GAMBLING'\n",
    "regions = [*range(360)]\n",
    "\n",
    "n_features     = 360  # this is number of parallel inputs\n",
    "n_timesteps    = 67   # this is number of timesteps\n",
    "n_hidden       = 20\n",
    "n_layers       = 3\n",
    "label_type     = 'WL'\n",
    "\n",
    "lr             = 1e-1\n",
    "train_episodes = 50\n",
    "\n",
    "n_trials       = 10\n",
    "\n",
    "df_list = []\n",
    "for i in range(n_trials):\n",
    "  df_list.append(training(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type))\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True) #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "MnJlEudRX9M6",
    "outputId": "0a6cffe0-3086-420f-8102-814f448d6c1b"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoc', y='accuracy', data=df, hue='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nnw_QE707j1i",
    "outputId": "0afdee1a-bf28-471d-caf2-5c13e6df4821"
   },
   "outputs": [],
   "source": [
    "print('Accuracy Test set = ',round(df[df['train']==False]['accuracy'].max(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZJqLkPkageD"
   },
   "outputs": [],
   "source": [
    "def training_hooks(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type):\n",
    "\n",
    "  if label_type=='flanker':\n",
    "    net = LSTMRegression2(n_features,n_timesteps, n_hidden, n_layers)\n",
    "  else:\n",
    "    net = LSTMRegression(n_features,n_timesteps, n_hidden, n_layers)\n",
    "\n",
    "  criterion =   torch.nn.MSELoss() # torch.nn.MSELoss() # reduction='sum' created huge loss value nn.CrossEntropyLoss() # torch.nn.NLLLoss() #\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "  label = {}\n",
    "  X, label['WL'], label['200'], label['40k'], label['flanker'], label['gender'] = load_data(experiment, regions)\n",
    "  y = label[label_type]\n",
    "  batch_size = 640\n",
    "\n",
    "  X_train = X[0:640,:,:]\n",
    "  y_train = y[0:640]\n",
    "\n",
    "  X_test = X[640:800,:,:]\n",
    "  y_test = y[640:800]\n",
    "\n",
    "  df_list = []\n",
    "\n",
    "  net.train()\n",
    "\n",
    "  for t in range(train_episodes):\n",
    "    for b in range(0,len(X_train),batch_size):\n",
    "\n",
    "      inpt = X_train[b:b+batch_size,:,:]\n",
    "      target = y_train[b:b+batch_size]\n",
    "\n",
    "      x_batch = inpt.clone().float()\n",
    "      y_batch = target.clone().float()\n",
    "\n",
    "      output = net(x_batch)\n",
    "      # if label_type=='flanker':\n",
    "      #   output=torch.log(output/(1-output+1e-8))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = criterion(output.view(-1), y_batch)\n",
    "      accuracy=((output.view(-1)>.5)==y_batch).sum()/640\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      net.eval()\n",
    "      y_hat = net(X_test.float())\n",
    "      # if label_type=='flanker':\n",
    "      #   y_hat=torch.log(y_hat/(1-y_hat+1e-8))\n",
    "\n",
    "      loss_val = criterion(y_hat.view(-1), y_test.float())\n",
    "      accuracy_val=((y_hat.view(-1)>.5)==y_test).sum()/(800-640)\n",
    "\n",
    "      if label_type=='WL' or label_type=='gender':\n",
    "        df_list.append({'epoc': t, 'accuracy': accuracy.item(), 'train':True})\n",
    "        df_list.append({'epoc': t, 'accuracy': accuracy_val.item(), 'train':False})\n",
    "      else:\n",
    "        df_list.append({'epoc': t, 'loss': loss.item(), 'train':True})\n",
    "        df_list.append({'epoc': t, 'loss': loss_val.item(), 'train':False})\n",
    "\n",
    "  return net, pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DM5sMB7bUQv",
    "outputId": "524af06d-1ed9-4e75-90f7-e1c3aaa80056"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment='GAMBLING'\n",
    "regions = [*range(360)]\n",
    "\n",
    "n_features     = 360  # this is number of parallel inputs\n",
    "n_timesteps    = 67   # this is number of timesteps\n",
    "n_hidden       = 20\n",
    "n_layers       = 3\n",
    "label_type     = 'WL'\n",
    "\n",
    "lr             = 1e-1\n",
    "train_episodes = 50\n",
    "\n",
    "n_trials       = 1\n",
    "\n",
    "df_list = []\n",
    "for i in range(n_trials):\n",
    "  net, df = training_hooks(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type)\n",
    "  df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True) #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFdEuZciaeej"
   },
   "outputs": [],
   "source": [
    "net.l_lstm.register_forward_hook(get_activation('l_lstm'))\n",
    "\n",
    "X, y, _, _, _, _ = load_data(experiment, regions)\n",
    "\n",
    "# print(X[0,:].unsqueeze(0).shape)\n",
    "output = net(X[0,:].unsqueeze(0).float())\n",
    "# print(activation['l_lstm'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "n6oM0mWwgVRd",
    "outputId": "33d636d0-3f33-4a37-8de7-8acb9c08c592"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# 200 300\n",
    "tsne = TSNE(perplexity=8, learning_rate=200)\n",
    "\n",
    "print(X[0,:].unsqueeze(0).shape)\n",
    "embeding_list=[]\n",
    "for i in range(800):\n",
    "  output = net(X[0,:].unsqueeze(0).float())\n",
    "  embeding_list.append(activation['l_lstm'][1][1][2])\n",
    "embeding = torch.concat(embeding_list).detach().numpy()\n",
    "\n",
    "X_tsne = tsne.fit_transform(embeding)\n",
    "\n",
    "plt.close()\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], alpha=.25, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ajuq6Wn6ZZ0_"
   },
   "outputs": [],
   "source": [
    "X, _, y, _, _, _ = load_data(experiment, regions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiqXANutU0Ky"
   },
   "outputs": [],
   "source": [
    "def transfer ():\n",
    "  for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  num_ftrs = net.l_linear.in_features\n",
    "  # print(num_ftrs)\n",
    "  net.l_linear = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "  lr             = 1e-3\n",
    "\n",
    "  criterion = torch.nn.MSELoss() # torch.nn.MSELoss() # reduction='sum' created huge loss value nn.CrossEntropyLoss() # torch.nn.NLLLoss() #\n",
    "  optimizer = torch.optim.Adam(net.l_linear.parameters(), lr=lr)\n",
    "\n",
    "  X, _, y, _, _, _ = load_data(experiment, regions)\n",
    "  batch_size = 640\n",
    "  train_episodes = 150\n",
    "  label_type=='WL'\n",
    "\n",
    "  y = (y>y.mean()).int()\n",
    "\n",
    "  X_train = X[0:640,:,:]\n",
    "  y_train = y[0:640]\n",
    "\n",
    "  X_test = X[640:800,:,:]\n",
    "  y_test = y[640:800]\n",
    "\n",
    "  df_list = []\n",
    "\n",
    "  net.train()\n",
    "\n",
    "  for t in range(train_episodes):\n",
    "    for b in range(0,len(X_train),batch_size):\n",
    "\n",
    "      inpt = X_train[b:b+batch_size,:,:]\n",
    "      target = y_train[b:b+batch_size]\n",
    "\n",
    "      x_batch = inpt.clone().float()\n",
    "      y_batch = target.clone().float()\n",
    "\n",
    "      output = net(x_batch)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = criterion(output.view(-1), y_batch)\n",
    "      accuracy=((output.view(-1)>.5)==y_batch).sum()/640\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      net.eval()\n",
    "      y_hat = net(X_test.float())\n",
    "\n",
    "      loss_val = criterion(y_hat.view(-1), y_test.float())\n",
    "      accuracy_val=((y_hat.view(-1)>.5)==y_test).sum()/(800-640)\n",
    "\n",
    "      if label_type=='WL' or label_type=='gender':\n",
    "        df_list.append({'epoc': t, 'accuracy': accuracy.item(), 'train':True})\n",
    "        df_list.append({'epoc': t, 'accuracy': accuracy_val.item(), 'train':False})\n",
    "      else:\n",
    "        df_list.append({'epoc': t, 'loss': loss.item(), 'train':True})\n",
    "        df_list.append({'epoc': t, 'loss': loss_val.item(), 'train':False})\n",
    "\n",
    "  return pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qv0nmngcsMwj"
   },
   "outputs": [],
   "source": [
    "n_trials       = 10\n",
    "\n",
    "df_list = []\n",
    "for i in range(n_trials):\n",
    "  df = transfer()\n",
    "  df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True) #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "U5IpbCLVc0Uz",
    "outputId": "e9de490e-bd09-465f-d324-c5b810b27ad5"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoc', y='accuracy', data=df, hue='train')\n",
    "plt.ylim([0.4,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56nWzZl9iGTP"
   },
   "outputs": [],
   "source": [
    "def training(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type):\n",
    "\n",
    "  if label_type=='flanker':\n",
    "    net = LSTMRegression2(n_features,n_timesteps, n_hidden, n_layers)\n",
    "  else:\n",
    "    net = LSTMRegression(n_features,n_timesteps, n_hidden, n_layers)\n",
    "\n",
    "  criterion =   torch.nn.MSELoss() # torch.nn.MSELoss() # reduction='sum' created huge loss value nn.CrossEntropyLoss() # torch.nn.NLLLoss() #\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "  label = {}\n",
    "  X, label['WL'], y, label['40k'], label['flanker'], label['gender'] = load_data(experiment, regions)\n",
    "  # y = label[label_type]\n",
    "  batch_size = 640\n",
    "\n",
    "  y = (y>y.mean()).int()\n",
    "\n",
    "  X_train = X[0:640,:,:]\n",
    "  y_train = y[0:640]\n",
    "\n",
    "  X_test = X[640:800,:,:]\n",
    "  y_test = y[640:800]\n",
    "\n",
    "  df_list = []\n",
    "\n",
    "  net.train()\n",
    "\n",
    "  for t in range(train_episodes):\n",
    "    for b in range(0,len(X_train),batch_size):\n",
    "\n",
    "      inpt = X_train[b:b+batch_size,:,:]\n",
    "      target = y_train[b:b+batch_size]\n",
    "\n",
    "      x_batch = inpt.clone().float()\n",
    "      y_batch = target.clone().float()\n",
    "\n",
    "      output = net(x_batch)\n",
    "      # if label_type=='flanker':\n",
    "      #   output=torch.log(output/(1-output+1e-8))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = criterion(output.view(-1), y_batch)\n",
    "      accuracy=((output.view(-1)>.5)==y_batch).sum()/640\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      net.eval()\n",
    "      y_hat = net(X_test.float())\n",
    "      # if label_type=='flanker':\n",
    "      #   y_hat=torch.log(y_hat/(1-y_hat+1e-8))\n",
    "\n",
    "      loss_val = criterion(y_hat.view(-1), y_test.float())\n",
    "      accuracy_val=((y_hat.view(-1)>.5)==y_test).sum()/(800-640)\n",
    "\n",
    "      if label_type=='WL' or label_type=='gender':\n",
    "        df_list.append({'epoc': t, 'accuracy': accuracy.item(), 'train':True})\n",
    "        df_list.append({'epoc': t, 'accuracy': accuracy_val.item(), 'train':False})\n",
    "      else:\n",
    "        df_list.append({'epoc': t, 'loss': loss.item(), 'train':True})\n",
    "        df_list.append({'epoc': t, 'loss': loss_val.item(), 'train':False})\n",
    "\n",
    "  return pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vt5tdNMniyH1",
    "outputId": "6e8398d6-94ce-4b59-b34d-360f9860e2b2"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment='GAMBLING'\n",
    "regions = [*range(360)]\n",
    "\n",
    "n_features     = 360  # this is number of parallel inputs\n",
    "n_timesteps    = 67   # this is number of timesteps\n",
    "n_hidden       = 20\n",
    "n_layers       = 3\n",
    "label_type     = 'WL'\n",
    "\n",
    "lr             = 1e-1\n",
    "train_episodes = 50\n",
    "\n",
    "n_trials       = 10\n",
    "\n",
    "df_list = []\n",
    "for i in range(n_trials):\n",
    "  df = training(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, lr, train_episodes, label_type)\n",
    "  df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True) #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "Kk2w_LNQi9v_",
    "outputId": "d023666b-1f31-41ef-cd46-215e65dfbba8"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoc', y='accuracy', data=df, hue='train')\n",
    "plt.ylim([0.4,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2j1PmTzmdg5",
    "outputId": "32353375-e71d-4166-e6ba-4fc08d833256"
   },
   "outputs": [],
   "source": [
    "print(embeding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5kuMaM54adU"
   },
   "outputs": [],
   "source": [
    "def trainingcond(experiment, regions, n_features, n_timesteps, n_hidden, n_layers, n_add, n_add_hidden, lr, train_episodes, label_type):\n",
    "\n",
    "  if label_type=='flanker':\n",
    "    net = LSTMRegressionCond2(n_features,n_timesteps, n_hidden, n_layers, n_add, n_add_hidden)\n",
    "  else:\n",
    "    net = LSTMRegressionCond(n_features,n_timesteps, n_hidden, n_layers, n_add, n_add_hidden)\n",
    "\n",
    "  criterion =   torch.nn.MSELoss() # torch.nn.MSELoss() # reduction='sum' created huge loss value nn.CrossEntropyLoss() # torch.nn.NLLLoss() #\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "  label = {}\n",
    "  X, label['WL'], label['200'], label['40k'], label['flanker'], label['gender'] = load_data(experiment, regions)\n",
    "  y = label[label_type]\n",
    "  batch_size = 640\n",
    "\n",
    "  X_train = X[0:640,:,:]\n",
    "  y_train = y[0:640]\n",
    "\n",
    "  X_test = X[640:800,:,:]\n",
    "  y_test = y[640:800]\n",
    "\n",
    "  df_list = []\n",
    "\n",
    "  net.train()\n",
    "\n",
    "  for t in range(train_episodes):\n",
    "    for b in range(0,len(X_train),batch_size):\n",
    "\n",
    "      inpt = X_train[b:b+batch_size,:,:]\n",
    "      target = y_train[b:b+batch_size]\n",
    "\n",
    "      x_batch = inpt.clone().float()\n",
    "      y_batch = target.clone().float()\n",
    "\n",
    "      output = net(x_batch)\n",
    "      # if label_type=='flanker':\n",
    "      #   output=torch.log(output/(1-output+1e-8))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss = criterion(output.view(-1), y_batch)\n",
    "      accuracy=((output.view(-1)>.5)==y_batch).sum()/640\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      net.eval()\n",
    "      y_hat = net(X_test.float())\n",
    "      # if label_type=='flanker':\n",
    "      #   y_hat=torch.log(y_hat/(1-y_hat+1e-8))\n",
    "\n",
    "      loss_val = criterion(y_hat.view(-1), y_test.float())\n",
    "      accuracy_val=((y_hat.view(-1)>.5)==y_test).sum()/(800-640)\n",
    "\n",
    "      if label_type=='WL' or label_type=='gender':\n",
    "        df_list.append({'epoc': t, 'accuracy': accuracy.item(), 'train':True})\n",
    "        df_list.append({'epoc': t, 'accuracy': accuracy_val.item(), 'train':False})\n",
    "      else:\n",
    "        df_list.append({'epoc': t, 'loss': loss.item(), 'train':True})\n",
    "        df_list.append({'epoc': t, 'loss': loss_val.item(), 'train':False})\n",
    "\n",
    "  return pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlJ1ZfSAq_nm",
    "outputId": "c8a6f09a-5240-41f4-d790-55e0d50e42c1"
   },
   "outputs": [],
   "source": [
    "n_features  = 360  # this is number of parallel inputs\n",
    "n_timesteps = 39 # this is number of timesteps\n",
    "n_hidden = 20\n",
    "n_layers=3\n",
    "lr = 1e-3\n",
    "\n",
    "net = LSTMRegression(n_features,n_timesteps, n_hidden, n_layers)\n",
    "print(net)\n",
    "\n",
    "criterion =   torch.nn.MSELoss() # torch.nn.MSELoss() # reduction='sum' created huge loss value nn.CrossEntropyLoss() # torch.nn.NLLLoss() #\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "train_episodes = 50\n",
    "batch_size = 640\n",
    "\n",
    "X_train = X[0:640,:,:]\n",
    "y_train = y[0:640]\n",
    "\n",
    "X_test = X[640:800,:,:]\n",
    "y_test = y[640:800]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "kcQgLsLRrodh",
    "outputId": "bd933b34-2368-47ca-af70-51b884ca3204"
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "loss_val_list = []\n",
    "net.train()\n",
    "\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X_train),batch_size):\n",
    "\n",
    "        inpt = X_train[b:b+batch_size,:,:]\n",
    "        target = y_train[b:b+batch_size]\n",
    "\n",
    "        x_batch = inpt.clone().float()\n",
    "        y_batch = target.clone().float()\n",
    "\n",
    "        output = net(x_batch)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output.view(-1), y_batch)\n",
    "        accuracy=((output.view(-1)>.5)==y_batch).sum()/320\n",
    "        loss_list.append(accuracy.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        net.eval()\n",
    "        y_hat = net(X_test.float())\n",
    "        loss_val = criterion(y_hat.view(-1), y_test.float())\n",
    "        accuracy_val=((y_hat.view(-1)>.5)==y_test).sum()/80\n",
    "        loss_val_list.append(accuracy_val.item())\n",
    "\n",
    "plt.plot(np.array(loss_list))\n",
    "plt.plot(np.array(loss_val_list))\n",
    "plt.show()\n",
    "print('loss : ' , accuracy.item())\n",
    "print('loss_val : ' , accuracy_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPEVzyhNfqVP",
    "outputId": "a0d46843-2114-42bc-bd64-a527ef303209"
   },
   "outputs": [],
   "source": [
    "HCP_data_frame = pd.read_csv('ddisc.csv')\n",
    "HCP_data_frame.head()\n",
    "y_auc_list = []\n",
    "for i, auc in enumerate(HCP_data_frame['DDisc_AUC_200']):\n",
    "  y_auc_list.append(auc)\n",
    "  y_auc_list.append(auc)\n",
    "  y_auc_list.append(auc)\n",
    "  y_auc_list.append(auc)\n",
    "y_auc200 = torch.tensor(y_auc_list)\n",
    "y_auc_list = []\n",
    "for i, auc in enumerate(HCP_data_frame['DDisc_AUC_40K']):\n",
    "  y_auc_list.append(auc)\n",
    "  y_auc_list.append(auc)\n",
    "  y_auc_list.append(auc)\n",
    "  y_auc_list.append(auc)\n",
    "y_auc40k = torch.tensor(y_auc_list)\n",
    "y_auc40k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qi1HLItmr2PL"
   },
   "outputs": [],
   "source": [
    "def r2_loss(output, target):\n",
    "    target_mean = torch.mean(target)\n",
    "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - output) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "yPmC7TiQlVAo",
    "outputId": "6d7934bf-eda7-47bd-a5ba-32cf51c29e85"
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "y_hat = net(X.float())\n",
    "print(torch.sqrt(criterion(y_hat.view(-1), y_auc200.float())))\n",
    "print(torch.sqrt(criterion(y_hat.view(-1), y_auc40k.float())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "1Huj-9D_tFvg",
    "outputId": "754c599a-f96a-45f7-b9db-a9ab017839a2"
   },
   "outputs": [],
   "source": [
    "n_features  = 360  # this is number of parallel inputs\n",
    "n_timesteps = 39 # this is number of timesteps\n",
    "\n",
    "net_200 = LSTMRegression(n_features,n_timesteps)\n",
    "print(net_200)\n",
    "\n",
    "criterion =   torch.nn.MSELoss() # torch.nn.MSELoss() # reduction='sum' created huge loss value nn.CrossEntropyLoss() # torch.nn.NLLLoss() #\n",
    "optimizer = torch.optim.Adam(net_200.parameters(), lr=1e-3)\n",
    "\n",
    "train_episodes = 50\n",
    "batch_size = 320\n",
    "\n",
    "X_train = X[0:320,:,:]\n",
    "y_train = y_auc200[0:320]\n",
    "\n",
    "X_test = X[320:400,:,:]\n",
    "y_test = y_auc200[320:400]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "OMvooeQetnWz",
    "outputId": "e605bbdf-54a5-4584-bdbf-1b001f45834c"
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "loss_val_list = []\n",
    "net.train()\n",
    "\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X_train),batch_size):\n",
    "\n",
    "        inpt = X_train[b:b+batch_size,:,:]\n",
    "        target = y_train[b:b+batch_size]\n",
    "\n",
    "        x_batch = inpt.clone().float()\n",
    "        y_batch = target.clone().float()\n",
    "\n",
    "        output = net_200(x_batch)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output.view(-1), y_batch)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        net.eval()\n",
    "        y_hat = net_200(X_test.float())\n",
    "        loss_val = criterion(y_hat.view(-1), y_test.float())\n",
    "        loss_val_list.append(loss_val.item())\n",
    "\n",
    "plt.plot(np.array(loss_list))\n",
    "plt.plot(np.array(loss_val_list))\n",
    "plt.show()\n",
    "print('loss : ' , torch.sqrt(loss).item())\n",
    "print('loss_val : ' , torch.sqrt(loss_val).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PR1ScH-Hv45V",
    "outputId": "2ca7a769-a53e-4f4e-9fca-d2943f139862"
   },
   "outputs": [],
   "source": [
    "n_features  = 360  # this is number of parallel inputs\n",
    "n_timesteps = 39 # this is number of timesteps\n",
    "\n",
    "net_40k = LSTMRegression(n_features,n_timesteps)\n",
    "print(net_40k)\n",
    "\n",
    "criterion =   torch.nn.MSELoss() # torch.nn.MSELoss() # reduction='sum' created huge loss value nn.CrossEntropyLoss() # torch.nn.NLLLoss() #\n",
    "optimizer = torch.optim.Adam(net_40k.parameters(), lr=1e-3)\n",
    "\n",
    "train_episodes = 30\n",
    "batch_size = 320\n",
    "\n",
    "X_train = X[0:320,:,:]\n",
    "y_train = y_auc40k[0:320]\n",
    "\n",
    "X_test = X[320:400,:,:]\n",
    "y_test = y_auc40k[320:400]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "dUB3M4EpwQIw",
    "outputId": "d2ce6f26-0b42-40ab-be0f-2d47e05ea212"
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "loss_val_list = []\n",
    "net.train()\n",
    "\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X_train),batch_size):\n",
    "\n",
    "        inpt = X_train[b:b+batch_size,:,:]\n",
    "        target = y_train[b:b+batch_size]\n",
    "\n",
    "        x_batch = inpt.clone().float()\n",
    "        y_batch = target.clone().float()\n",
    "\n",
    "        output = net_40k(x_batch)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output.view(-1), y_batch)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        net.eval()\n",
    "        y_hat = net_40k(X_test.float())\n",
    "        loss_val = criterion(y_hat.view(-1), y_test.float())\n",
    "        loss_val_list.append(loss_val.item())\n",
    "\n",
    "plt.plot(np.array(loss_list))\n",
    "plt.plot(np.array(loss_val_list))\n",
    "plt.show()\n",
    "print('loss : ' , torch.sqrt(loss).item())\n",
    "print('loss_val : ' , torch.sqrt(loss_val).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dY2bgcZ1Dlh6",
    "outputId": "ccf4399e-0cd9-41b9-be96-4b2aa5ea2618"
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "y_hat = net(X_test.float())\n",
    "criterion(y_hat.view(-1), y_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w15c-R1xUIUw"
   },
   "outputs": [],
   "source": [
    "class HCP_dataset(Dataset):\n",
    "    \"\"\"HCP_dataset\"\"\"\n",
    "    # To-Do add brain region and other variables age & sex\n",
    "    def __init__(self, root_dir, target, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.HCP_data_frame = pd.read_excel(root_dir + '/HCP_Behavioral_2.0.xlsx').dropna(subset=[target]).reset_index()\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target = target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.HCP_data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # loaded variables\n",
    "        all_targets = self.HCP_data_frame[self.target][idx] # DDisc_AUC_40K DDisc_AUC_200\n",
    "\n",
    "        timeseries = load_timeseries(subject=idx if idx < 39 else idx+1,\n",
    "                             name=\"gambling\",\n",
    "                             dir=os.path.join(HCP_DIR, \"hcp_task\"),\n",
    "                             runs=1)\n",
    "\n",
    "        # transformed variables\n",
    "        xfm_all_targets = torch.tensor(all_targets)\n",
    "        xfm_timeseries = torch.tensor(timeseries)\n",
    "\n",
    "        # mean and std of data\n",
    "        mean = xfm_timeseries.mean()\n",
    "        std = xfm_timeseries.std()\n",
    "\n",
    "        # normalize dataset\n",
    "        xfm_timeseries = (xfm_timeseries.float() - mean) / std\n",
    "        xfm_all_targets = xfm_all_targets.float()\n",
    "\n",
    "        # match input and targets ##### TO-DO this is the remove sub_ID 109830 == #39 ##### DONE\n",
    "\n",
    "        sample = {'timeseries': xfm_timeseries[[164, 344]], 'all_targets': xfm_all_targets}\n",
    "\n",
    "        if self.transform:\n",
    "            #sample = {'timeseries': self.transform(xfm_timeseries), 'all_targets': self.transform(xfm_all_targets)}\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNkoziSDonBO"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input, hidden, layers, labels):\n",
    "\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input, hidden, layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, labels)\n",
    "\n",
    "    def forward(self, x, x_len, max_length=None):\n",
    "        x, _ = self.lstm(x) # h0, c0 initialized to zero;\n",
    "        y = self.fc(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "a83JzJYrJO9_"
   },
   "source": [
    "For the motor task, this evs variable contains a list of 5 arrays corresponding to the 5 conditions.\n",
    "\n",
    "Now let's use these evs to compare the average activity during the left foot ('lf') and right foot ('rf') conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "id": "PRqAatxhJO9_"
   },
   "outputs": [],
   "source": [
    "# we need a little function that averages all frames from any given condition\n",
    "def average_frames(data, evs, experiment, cond):\n",
    "  idx = EXPERIMENTS[experiment]['cond'].index(cond)\n",
    "  return np.mean(np.concatenate([np.mean(data[:, evs[idx][i]], axis=1, keepdims=True) for i in range(len(evs[idx]))], axis=-1), axis=1)\n",
    "\n",
    "\n",
    "loss_activity = average_frames(data, evs, my_exp, 'loss')\n",
    "win_activity = average_frames(data, evs, my_exp, 'win')\n",
    "contrast = loss_activity - win_activity  # difference between left and right hand movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "execution": {},
    "id": "h0M3vicEJO9_",
    "outputId": "00f2be9a-8ec9-44ca-ffff-be7019526409"
   },
   "outputs": [],
   "source": [
    "# Plot activity level in each ROI for both conditions\n",
    "plt.plot(loss_activity,label='loss')\n",
    "plt.plot(win_activity,label='win')\n",
    "plt.xlabel('ROI')\n",
    "plt.ylabel('activity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "p2i_s7ieJO9_"
   },
   "source": [
    "Now let's plot these activity vectors. We will also make use of the ROI names to find out which brain areas show highest activity in these conditions. But since there are so many areas, we will group them by network.\n",
    "\n",
    "A powerful tool for organising and plotting this data is the combination of pandas and seaborn. Below is an example where we use pandas to create a table for the activity data and we use seaborn oto visualise it.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "execution": {},
    "id": "DUinXpT9JO-A",
    "outputId": "39f1c9b7-3042-4609-aeaa-c13874945052"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'loss_activity' : loss_activity,\n",
    "                   'win_activity' : win_activity,\n",
    "                   'network' : region_info['network'],\n",
    "                   'hemi' : region_info['hemi']})\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "sns.barplot(y='network', x='loss_activity', data=df, hue='hemi',ax=ax1)\n",
    "sns.barplot(y='network', x='win_activity', data=df, hue='hemi',ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "iUi--z2jJO-A"
   },
   "source": [
    "You should be able to notice that for the somatosensory network, brain activity in the right hemi is higher for the left foot movement and vice versa for the left hemi and right foot. But this may be subtle at the single subject/session level (these are quick 3-4min scans).\n",
    "\n",
    "\n",
    "Let us boost thee stats by averaging across all subjects and runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "id": "sIhWnZexJO-A"
   },
   "outputs": [],
   "source": [
    "group_contrast = 0\n",
    "for s in subjects:\n",
    "  for r in [0, 1]:\n",
    "    data = load_single_timeseries(subject=s, experiment=my_exp,\n",
    "                                  run=r, remove_mean=True)\n",
    "    evs = load_evs(subject=s, experiment=my_exp,run=r)\n",
    "\n",
    "    loss_activity = average_frames(data, evs, my_exp, 'win')\n",
    "    win_activity = average_frames(data, evs, my_exp, 'loss')\n",
    "\n",
    "    contrast = loss_activity - win_activity\n",
    "    group_contrast += contrast\n",
    "\n",
    "group_contrast /= (len(subjects)*2)  # remember: 2 sessions per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "execution": {},
    "id": "rQYAR2xcJO-A",
    "outputId": "a28e8db8-f4a5-404c-8319-294787c95d8b"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'contrast' : group_contrast,\n",
    "                   'network' : region_info['network'],\n",
    "                   'hemi' : region_info['hemi']\n",
    "                   })\n",
    "# we will plot the left foot minus right foot contrast so we only need one plot\n",
    "plt.figure()\n",
    "sns.barplot(y='network', x='contrast', data=df, hue='hemi')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "OKUxK9wCJO-B"
   },
   "source": [
    "# Visualising the results on a brain\n",
    "\n",
    "Finally, we will visualise these resuts on the cortical surface of an average brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "c8FBQLEtJO-B"
   },
   "outputs": [],
   "source": [
    "# @title NMA provides an atlas. Run this cell to download it\n",
    "import os, requests\n",
    "\n",
    "# NMA provides an atlas\n",
    "fname = f\"{HCP_DIR}/atlas.npz\"\n",
    "url = \"https://osf.io/j5kuc/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "\n",
    "with np.load(fname) as dobj:\n",
    "  atlas = dict(**dobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "execution": {},
    "id": "t4DwlAIxJO-B",
    "outputId": "a07bcc05-929b-442d-c352-20374c4811cf"
   },
   "outputs": [],
   "source": [
    "# This uses the nilearn package\n",
    "from nilearn import plotting, datasets\n",
    "\n",
    "# Try both hemispheres (L->R and left->right)\n",
    "fsaverage = datasets.fetch_surf_fsaverage()\n",
    "surf_contrast = group_contrast[atlas[\"labels_L\"]]\n",
    "plotting.view_surf(fsaverage['infl_left'],\n",
    "                   surf_contrast,\n",
    "                   vmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylzjiEHx9Dt"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([x for x in range(0,100,10)])\n",
    "in_seq2 = array([x for x in range(5,105,10)])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buLFOv7PyA3z"
   },
   "outputs": [],
   "source": [
    "class LSTMRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,n_features,seq_length):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 40 # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "\n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features,\n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers,\n",
    "                                 batch_first = True)\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x, (self.hidden, _) = self.l_lstm(x)\n",
    "        return self.l_linear(x[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_Qo0NofyK9t"
   },
   "outputs": [],
   "source": [
    "n_features = 2 # this is number of parallel inputs\n",
    "n_timesteps = 3 # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "X, y = split_sequences(dataset, n_timesteps)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# create NN\n",
    "mv_net = MV_LSTM(n_features,n_timesteps)\n",
    "print(mv_net)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "\n",
    "train_episodes = 6000\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VAQ3grlyfrB"
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "mv_net.train()\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]\n",
    "        # print(\"input \",inpt.shape)\n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32)\n",
    "        y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "\n",
    "    #    mv_net.init_hidden(x_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)\n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch)\n",
    "        # print(\"output \",output.shape)\n",
    "        # print(\"y_batch \",y_batch.shape)\n",
    "        loss = criterion(output.view(-1), y_batch)\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    # print('step : ' , t , 'loss : ' , loss.item())\n",
    "plt.plot(np.array(loss_list))\n",
    "plt.show()\n",
    "print('loss : ' , loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYOdFG3LdOYa"
   },
   "outputs": [],
   "source": [
    "print(torch.tensor(X[0]).unsqueeze(0))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KB41NpMvdCkY"
   },
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "  print(mv_net(torch.tensor(X[i],dtype=torch.float).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tguDb4uhpy8"
   },
   "outputs": [],
   "source": [
    "print(mv_net(torch.tensor([[ 3,  8],\n",
    "         [13, 18],\n",
    "         [23, 28]],dtype=torch.float).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lvuStRC8S1S"
   },
   "outputs": [],
   "source": [
    "for b in range(0,len(X),batch_size):\n",
    "  print(b+batch_size)\n",
    "  print(X.shape)\n",
    "  # print(X[b:b+batch_size,:,:])\n",
    "print(X)\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hLXqJ7mUJO96",
    "kv__YxLCJO97",
    "OD2vIz1IJO98",
    "OKUxK9wCJO-B"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "brain2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
